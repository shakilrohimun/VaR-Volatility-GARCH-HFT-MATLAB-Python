% Assuming 'out3' contains 'day', 'tick', and 'logprice'

% Initialize results
logReturns = []; % Store log returns for the selected method
timeWeightedReturns = []; % Store time-weighted returns for the selected method
dailyLogReturns = {}; % Cell to store log returns for each day
cumulativeLogReturns = {}; % Cell to store cumulative log returns for each day

% Step 1: Split Data by Day to Avoid Jumps Between Days
uniqueDays = unique(out3.day);
dailyData = {}; % Store daily data for each day

for i = 1:length(uniqueDays)
    dailyData{i} = out3(out3.day == uniqueDays(i), :); % Extract data for each day
end

% Select the method to run
useBrownianBridge = true;     % Set to true to apply Brownian Bridge method

% Define parameters for sampling
samplingInterval = 0.0001; % Define sampling interval in ticks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%. Brownian Bridge %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

if useBrownianBridge
    for i = 1:length(dailyData)
        dailyPrices = dailyData{i}.logprice;
        dailyTicks = dailyData{i}.tick;

        % Calculate variance of log prices
        priceVariance = var(dailyPrices);

        overallTimeGrid = [];
        overallBridgePrices = [];

        % Iterate over daily prices to create Brownian Bridge for each segment
        for j = 1:length(dailyPrices)-1
            s1 = dailyPrices(j);
            s2 = dailyPrices(j+1);
            t1 = dailyTicks(j);
            t2 = dailyTicks(j+1);

            % Time grid for the segment
            segmentTimeGrid = linspace(t1, t2, round((t2 - t1) / samplingInterval) + 1);
            bridgeSegment = zeros(size(segmentTimeGrid));
            bridgeSegment(1) = s1;

            for k = 2:length(segmentTimeGrid)
                t = segmentTimeGrid(k);
                meanValue = s1 + (t - t1) / (t2 - t1) * (s2 - s1);
                variance = (t2 - t) * (t - t1) / (t2 - t1);
                increment = randn * sqrt(priceVariance * (variance));
                bridgeSegment(k) = meanValue + increment;
            end

            bridgeSegment(end) = s2;

            overallTimeGrid = [overallTimeGrid, segmentTimeGrid];
            overallBridgePrices = [overallBridgePrices, bridgeSegment];
        end

        % Calculate log returns for the Brownian bridge
        bridgeLogReturns = diff(overallBridgePrices);
        dailyLogReturns{i} = bridgeLogReturns; % Store Brownian bridge log returns
    end
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% Accumulate High-Frequency Log Returns %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

for i = 1:length(dailyLogReturns)
    if ~isempty(dailyLogReturns{i})
        cumulativeLogReturns{i} = cumsum(dailyLogReturns{i}); % Calculate cumulative log returns
    else
        cumulativeLogReturns{i} = []; % Handle cases with no data
    end
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% Plot Cumulative Log Returns for Each Day %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

for i = 1:3 % Limit to the first 3 days for visualization
    if ~isempty(cumulativeLogReturns{i})
        figure;
        plot(cumulativeLogReturns{i}, 'b-', 'DisplayName', 'Cumulative Log Returns');
        title(sprintf('Day %d: Cumulative Log Returns', uniqueDays(i)));
        xlabel('Tick Number');
        ylabel('Cumulative Log Returns');
        grid on;
    end
end
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% FPCA on Cumulative Log Returns %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Step 1: Interpolate Data and Represent in Fourier Basis
numDays = length(cumulativeLogReturns);
ticksPerDay = min(cellfun(@length, cumulativeLogReturns)); % Ensure consistent dimensions
cumulativeLogReturns = cellfun(@(x) x(1:ticksPerDay), cumulativeLogReturns, 'UniformOutput', false);

% Combine all days into a matrix
dataMatrix = cell2mat(cumulativeLogReturns');

% Estimate the mean function (average across days)
meanFunction = mean(dataMatrix, 2);

% Center the data by subtracting the mean function
centeredData = dataMatrix - meanFunction;

% Define the Fourier basis
nbasis = 20; % Number of Fourier basis functions to use
fourierBasisFunctions = zeros(ticksPerDay, nbasis);
timePoints = linspace(0, 1, ticksPerDay)'; % Time points normalized between 0 and 1
for k = 1:nbasis
    if mod(k, 2) == 0
        % Cosine basis
        fourierBasisFunctions(:, k) = cos((k/2) * pi * timePoints);
    else
        % Sine basis
        fourierBasisFunctions(:, k) = sin(((k + 1)/2) * pi * timePoints);
    end
end

% Step 2: Project centered data onto Fourier basis to get coefficients
fourierCoefficients = centeredData * fourierBasisFunctions;

% Step 3: Estimate Mean Function and Center Data
meanCoefficients = mean(fourierCoefficients, 1);
centeredCoefficients = fourierCoefficients - meanCoefficients;

% Step 4: Estimate Covariance Matrix of Fourier Coefficients
covarianceMatrix = (centeredCoefficients' * centeredCoefficients) / numDays;

% Step 5: Perform Eigen Decomposition on Covariance Matrix
[eigenVectors, eigenValuesMatrix] = eig(covarianceMatrix);

% Extract eigenvalues and sort them in descending order
eigenValues = diag(eigenValuesMatrix);
[sortedEigenValues, indices] = sort(eigenValues, 'descend');
eigenVectors = eigenVectors(:, indices);

% Select the first few eigenfunctions to use for FPCA
numComponents = 4;
eigenVectors = eigenVectors(:, 1:numComponents);
sortedEigenValues = sortedEigenValues(1:numComponents);

% Reconstruct eigenfunctions in the original domain
eigenFunctions = fourierBasisFunctions * eigenVectors;

% Ensure eigenfunctions start at zero
for i = 1:numComponents
    eigenFunctions(:, i) = eigenFunctions(:, i) - eigenFunctions(1, i);
end

% Step 6: Project the centered coefficients onto the eigenvectors to get principal component scores
scores = centeredCoefficients * eigenVectors;

% Plot the First Four Principal Components on the Same Plot
figure;
plot(scores(:, 1), 'r-', 'LineWidth', 1.5, 'DisplayName', 'Principal Component 1');
hold on;
plot(scores(:, 2), 'b-', 'LineWidth', 1.5, 'DisplayName', 'Principal Component 2');
plot(scores(:, 3), 'g-', 'LineWidth', 1.5, 'DisplayName', 'Principal Component 3');
plot(scores(:, 4), 'k-', 'LineWidth', 1.5, 'DisplayName', 'Principal Component 4');
hold off;
title('First Four Principal Components of Smoothed Cumulative Log Returns');
xlabel('Day Number');
ylabel('Principal Component Value');
legend;
grid on;

% Plot the First Few Eigenfunctions on the Same Plot
figure;
plot(eigenFunctions(:, 1), 'r-', 'LineWidth', 1.5, 'DisplayName', 'Eigenfunction 1');
hold on;
plot(eigenFunctions(:, 2), 'b-', 'LineWidth', 1.5, 'DisplayName', 'Eigenfunction 2');
plot(eigenFunctions(:, 3), 'g-', 'LineWidth', 1.5, 'DisplayName', 'Eigenfunction 3');
plot(eigenFunctions(:, 4), 'k-', 'LineWidth', 1.5, 'DisplayName', 'Eigenfunction 4');
hold off;
title('First Four Eigenfunctions');
xlabel('Tick Number');
ylabel('Eigenfunction Value');
legend;
grid on;

% Display the Principal Components
disp('Principal Components (first 3):');
disp(scores(:, 1:3));
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% Normality Test and White Noise Assessment %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Step 7: Test Normality of Principal Component Scores
for k = 1:numComponents
    [h, pValue] = kstest((scores(:, k) - mean(scores(:, k))) / std(scores(:, k))); % Normality test on standardized scores
    fprintf('Normality test for Principal Component %d: h = %d, p-value = %.4f\n', k, h, pValue);
    if h == 0
        fprintf('Principal Component %d follows a normal distribution (p-value = %.4f)\n', k, pValue);
    else
        fprintf('Principal Component %d does not follow a normal distribution (p-value = %.4f)\n', k, pValue);
    end
end

% Step 8: Assess White Noise Property (Autocorrelation Function)
for k = 1:numComponents
    figure;
    autocorr(scores(:, k), 'NumLags', 20);
    title(sprintf('Autocorrelation of Principal Component %d', k));
    xlabel('Lag');
    ylabel('Autocorrelation');
end

% Estimate noise variance as the average variance of the residuals after reconstructing the data
reconstructedData = eigenFunctions * scores';
residuals = centeredData - reconstructedData';
noiseVariance = mean(var(residuals, 0, 2));
fprintf('Estimated noise variance: %.4f\n', noiseVariance);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% Quality of Fit Assessment as Function of p %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Step 9: Assess the Quality of the Fit by Varying p
explainedVariance = cumsum(sortedEigenValues) / sum(sortedEigenValues);

% Plot the Cumulative Explained Variance
figure;
plot(1:length(explainedVariance), explainedVariance, 'b-o', 'LineWidth', 1.5);
title('Cumulative Explained Variance as Function of p');
xlabel('Number of Components (p)');
ylabel('Cumulative Explained Variance');
grid on;

% Display the explained variance for different values of p
fprintf('Explained Variance for different values of p:\n');
for p = 1:length(explainedVariance)
    fprintf('p = %d: %.4f\n', p, explainedVariance(p));
end
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% Time Series Modeling for Forecasting %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Step 9: Split the Data into Training and Testing Sets (70%-30% Split)
nTrain = floor(0.7 * numDays);
trainScores = scores(1:nTrain, :);
testScores = scores(nTrain+1:end, :);

% Step 10: Forecasting Using Custom AR(1) Model
forecastHorizon = 5; % Number of steps to forecast
predictedScores = zeros(forecastHorizon, numComponents);

for k = 1:numComponents
    % Standardize the data to help with scaling issues
    data = (trainScores(:, k) - mean(trainScores(:, k))) / std(trainScores(:, k)); 
    T = length(data);

    % Define initial parameters [alpha, phi, sigma^2] for AR(1)
    initialGuess = [mean(data), 0.5, var(data)]; % [Intercept, AR coefficient, Variance]

    % Log-likelihood function for AR(1)
    logLikelihood = @(params) -(-T/2 * log(2*pi*params(3)) ...
        - sum((data(2:end) - params(1) - params(2) * data(1:end-1)).^2) / (2 * params(3)));

    % Optimize to find parameters that maximize the log-likelihood
    options = optimset('fminunc');
    [paramsOpt, ~, exitflag, output, ~, hessian] = fminunc(logLikelihood, initialGuess, options);

    % Check if optimization converged
    if exitflag <= 0
        disp(['Warning: Optimization for Component ', num2str(k), ' may not have converged.']);
        disp(output.message);
    end

    % Extract estimated parameters
    alpha = paramsOpt(1); % Intercept
    phi = paramsOpt(2);   % AR(1) coefficient
    sigma2 = paramsOpt(3); % Variance of noise

    % Calculate Standard Errors, T-Statistics, and P-Values
    try
        % Calculate standard errors from the Hessian, if available
        stdErrors = sqrt(diag(inv(hessian)));
        tStats = paramsOpt ./ stdErrors;
        pValues = 2 * (1 - normcdf(abs(tStats))); % Two-tailed p-value
    catch
        disp(['Warning: Hessian inversion failed for Component ', num2str(k), '.']);
        stdErrors = NaN(size(paramsOpt));
        tStats = NaN(size(paramsOpt));
        pValues = NaN(size(paramsOpt));
    end

    % Display results for this component
    fprintf('\nResults for Principal Component %d:\n', k);
    resultsTable = table(paramsOpt', stdErrors, tStats, pValues, ...
        'VariableNames', {'Value', 'StandardError', 'TStatistic', 'PValue'}, ...
        'RowNames', {'Constant', 'AR{1}', 'Variance'});
    disp(resultsTable);

    % Forecast future values based on the estimated parameters
    yF = zeros(forecastHorizon, 1);
    yF(1) = alpha + phi * data(end); % Initial forecast using last data point
    
    for t = 2:forecastHorizon
        yF(t) = alpha + phi * yF(t-1);
    end
    predictedScores(:, k) = yF * std(trainScores(:, k)) + mean(trainScores(:, k)); % Unscale forecasted scores
end

% Extend the mean function for the forecast horizon
meanFunctionExtended = repmat(meanFunction, 1, forecastHorizon);

% Reconstruct the Forecasted Data using the Functional Model
eigenFunctionsResampled = interp1(linspace(0, 1, size(eigenFunctions, 1)), ...
                                  eigenFunctions(:, 1:numComponents), ...
                                  linspace(0, 1, size(meanFunctionExtended, 1)));
reconstructedForecasts = meanFunctionExtended + eigenFunctionsResampled * predictedScores';

% Plot the Forecasted Data
figure;
plot(reconstructedForecasts, 'LineWidth', 1.5);
title('Forecasted Data for Next 5 Steps');
xlabel('Tick Number');
ylabel('Forecasted Value');
grid on;


% Evaluate Prediction Performance
for k = 1:numComponents
    % Calculate forecasts over test data length using the estimated parameters
    yPred = zeros(length(testScores(:, k)), 1);
    yPred(1) = alpha + phi * data(end); % Initial prediction using last training data point
    for t = 2:length(testScores(:, k))
        yPred(t) = alpha + phi * yPred(t-1);
    end
    
    % Unscale the predicted values back to the original scale
    yPred = yPred * std(trainScores(:, k)) + mean(trainScores(:, k));
    
    residuals = testScores(:, k) - yPred;
    mse = mean(residuals.^2);
    fprintf('Mean Squared Error for Principal Component %d: %.4f\n', k, mse);
end
